Seattle area residents use the 'Find It, Fix It' (FIFI) mobile app to report non-emergency problems to the City, _e.g._ graffiti, broken streetlights, and etc. This project includes: data exploration, statistical analysis, and classification models, using NLP and image classification. A single end-to-end network was trained to predict multiple class labels from a multi-input, mixed data model using TensorFlow and Keras.  

The image classifier model was built upon the pre-trained InceptionV3 model, so lower layer feature extractions correspond to the original 1000 classes which do not necessarily contain our image class labels. The NLP classifier included learned word embeddings, and a recurrent neural network with a bidirectional LSTM layer (nodes ranged from 10-128). To improve training efficiency, the cumbersome RNN model was compared to the fastText classification model with bigrams, as developed by Facebook AI Research. (Joulin _et al._, 2016)  

### Project Objective
To assist the City by training a classification filter based on the image and text data contained in each FIFI request. Using both text and image data is important because the data is very noisy and some of the class categories are difficult to separate due to the overlap between request categories.  

### About the Data
The data was generated by the FIFI mobile app produced by the City of Seattle's Customer Service Bureau. The data was requested through a public information request form on the [City Public Disclosure site](http://www.seattle.gov/public-records). All reports include text, and most include photos.  

![fifi app home screen](images/fifi_app_home_screen.png)  

### Project Challenges
• Ambiguous Classes: overlapping categories, with imbalanced categorical data  
• Noisy Data: incomplete, incorrectly labeled, and duplicate data  
• Data Volume: Scale of image data (> 50 GB)  

### Project Results
Due to the high degree of ambiguity between classes, some categories performed better than others. For example, the text classifier accurately identifies Graffiti and Garbage category instances, but tends to incorrectly label Abandoned Vehicle and Parking instances. This is understandable since those categories are closely related.  
The fastText model performed slightly worse than the best RNN models. However, fastText only took about 30 minutes to train on 152,820 text samples instead of the 10 hours for the RNN.  
